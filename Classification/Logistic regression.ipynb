{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression :\n",
    "is a variation of Linear Regression, useful when the observed dependent variable, y, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>tollmon</th>\n",
       "      <th>equipmon</th>\n",
       "      <th>cardmon</th>\n",
       "      <th>wiremon</th>\n",
       "      <th>longten</th>\n",
       "      <th>tollten</th>\n",
       "      <th>cardten</th>\n",
       "      <th>voice</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>20.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.25</td>\n",
       "      <td>35.7</td>\n",
       "      <td>42.00</td>\n",
       "      <td>211.45</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>45.00</td>\n",
       "      <td>50.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>64.9</td>\n",
       "      <td>239.55</td>\n",
       "      <td>1873.05</td>\n",
       "      <td>880.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.45</td>\n",
       "      <td>166.10</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.35</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>973.10</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.854</td>\n",
       "      <td>3.199</td>\n",
       "      <td>4.419</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>34.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.25</td>\n",
       "      <td>959.40</td>\n",
       "      <td>435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.792</td>\n",
       "      <td>3.332</td>\n",
       "      <td>3.178</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>23.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.90</td>\n",
       "      <td>128.45</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.168</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>47.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.75</td>\n",
       "      <td>64.0</td>\n",
       "      <td>186.60</td>\n",
       "      <td>1152.90</td>\n",
       "      <td>780.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.163</td>\n",
       "      <td>3.866</td>\n",
       "      <td>3.219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>61.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.55</td>\n",
       "      <td>26.50</td>\n",
       "      <td>44.1</td>\n",
       "      <td>1063.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>3.240</td>\n",
       "      <td>5.247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0      11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1      33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2      23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3      38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4       7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "..      ...   ...      ...     ...  ...     ...    ...       ...       ...   \n",
       "195    55.0  44.0     24.0    83.0  1.0    23.0    0.0       1.0       0.0   \n",
       "196    34.0  23.0      3.0    24.0  1.0     7.0    0.0       1.0       0.0   \n",
       "197     6.0  32.0     10.0    47.0  1.0    10.0    0.0       1.0       0.0   \n",
       "198    24.0  30.0      0.0    25.0  4.0     5.0    0.0       1.0       1.0   \n",
       "199    61.0  50.0     16.0   190.0  2.0    22.0    1.0       1.0       1.0   \n",
       "\n",
       "     longmon  tollmon  equipmon  cardmon  wiremon  longten  tollten  cardten  \\\n",
       "0       4.40    20.75      0.00    15.25     35.7    42.00   211.45    125.0   \n",
       "1       9.45     0.00      0.00     0.00      0.0   288.80     0.00      0.0   \n",
       "2       6.30     0.00      0.00     0.00      0.0   157.05     0.00      0.0   \n",
       "3       6.05    45.00     50.10    23.25     64.9   239.55  1873.05    880.0   \n",
       "4       7.10    22.00      0.00    23.75      0.0    47.45   166.10    145.0   \n",
       "..       ...      ...       ...      ...      ...      ...      ...      ...   \n",
       "195    17.35    24.50      0.00    14.25      0.0   973.10  1343.50    720.0   \n",
       "196     6.00    28.00      0.00    12.75      0.0   203.25   959.40    435.0   \n",
       "197     3.85    23.75      0.00    12.50      0.0    29.90   128.45     80.0   \n",
       "198     8.70    47.75      0.00    32.75     64.0   186.60  1152.90    780.0   \n",
       "199    16.85     0.00     42.55    26.50     44.1  1063.15     0.00   1600.0   \n",
       "\n",
       "     voice  pager  internet  callwait  confer  ebill  loglong  logtoll  lninc  \\\n",
       "0      1.0    1.0       0.0       1.0     1.0    0.0    1.482    3.033  4.913   \n",
       "1      0.0    0.0       0.0       0.0     0.0    0.0    2.246    3.240  3.497   \n",
       "2      0.0    0.0       0.0       0.0     1.0    0.0    1.841    3.240  3.401   \n",
       "3      1.0    1.0       1.0       1.0     1.0    1.0    1.800    3.807  4.331   \n",
       "4      1.0    0.0       0.0       1.0     1.0    0.0    1.960    3.091  4.382   \n",
       "..     ...    ...       ...       ...     ...    ...      ...      ...    ...   \n",
       "195    0.0    0.0       0.0       0.0     1.0    0.0    2.854    3.199  4.419   \n",
       "196    0.0    0.0       0.0       1.0     1.0    0.0    1.792    3.332  3.178   \n",
       "197    0.0    0.0       0.0       1.0     1.0    0.0    1.348    3.168  3.850   \n",
       "198    1.0    1.0       1.0       1.0     1.0    1.0    2.163    3.866  3.219   \n",
       "199    0.0    0.0       1.0       0.0     0.0    1.0    2.824    3.240  5.247   \n",
       "\n",
       "     custcat  churn  \n",
       "0        4.0    1.0  \n",
       "1        1.0    1.0  \n",
       "2        3.0    0.0  \n",
       "3        4.0    0.0  \n",
       "4        3.0    0.0  \n",
       "..       ...    ...  \n",
       "195      3.0    0.0  \n",
       "196      3.0    0.0  \n",
       "197      3.0    0.0  \n",
       "198      4.0    1.0  \n",
       "199      2.0    0.0  \n",
       "\n",
       "[200 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-Coursera/labs/Data_files/ChurnData.csv\")\n",
    "pd.set_option(\"display.max_columns\",28)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing and selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]\n",
    "df['churn']=df['churn'].astype('int')\n",
    "df['churn'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14, -0.63, -0.46, ...,  1.7 , -0.58, -0.86],\n",
       "       [-0.12, -0.63,  0.03, ..., -0.64, -1.14, -0.86],\n",
       "       [-0.58, -0.86, -0.26, ..., -1.42, -0.92, -0.86],\n",
       "       ...,\n",
       "       [-1.37, -0.7 , -0.16, ..., -1.42, -0.03, -0.86],\n",
       "       [-0.53, -0.86, -1.15, ...,  0.92, -0.58, -0.86],\n",
       "       [ 1.18,  0.68,  0.43, ..., -0.64,  1.32,  1.16]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalizing the data\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (160, 7) (160,)\n",
      "Test set: (40, 7) (40,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "The LogisticRegression function implements logistic regression and can use different numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.\n",
    "\n",
    "* The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = LR.predict(X_test)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54, 0.46],\n",
       "       [0.61, 0.39],\n",
       "       [0.56, 0.44],\n",
       "       [0.63, 0.37],\n",
       "       [0.56, 0.44],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.61, 0.39],\n",
       "       [0.41, 0.59],\n",
       "       [0.63, 0.37],\n",
       "       [0.58, 0.42],\n",
       "       [0.63, 0.37],\n",
       "       [0.48, 0.52],\n",
       "       [0.43, 0.57],\n",
       "       [0.66, 0.34],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.49, 0.51],\n",
       "       [0.49, 0.51],\n",
       "       [0.52, 0.48],\n",
       "       [0.62, 0.38],\n",
       "       [0.53, 0.47],\n",
       "       [0.64, 0.36],\n",
       "       [0.52, 0.48],\n",
       "       [0.51, 0.49],\n",
       "       [0.71, 0.29],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.52, 0.48],\n",
       "       [0.71, 0.29],\n",
       "       [0.68, 0.32],\n",
       "       [0.51, 0.49],\n",
       "       [0.42, 0.58],\n",
       "       [0.71, 0.29],\n",
       "       [0.6 , 0.4 ],\n",
       "       [0.64, 0.36],\n",
       "       [0.4 , 0.6 ],\n",
       "       [0.52, 0.48],\n",
       "       [0.66, 0.34],\n",
       "       [0.51, 0.49]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_prob = LR.predict_proba(X_test)\n",
    "ypred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard index for accuracy evaluation:\n",
    "\n",
    "we can define jaccard as the size of the intersection divided by the size of the union of two label sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "cm = confusion_matrix(ypred, y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# print(\"Confusion matrix:\\n\",cm)\n",
    "sns.heatmap(cm, annot=True ,cmap=plt.cm.Blues)\n",
    "plt.xticks(np.arange(2), ['churn=0','churn=1'])\n",
    "plt.yticks(np.arange(2), ['churn=0','churn=1'],rotation=45)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.tight_layout()\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the customers with churn value 0? Lets look at the 1st column. It looks like there were 25 customers whom their churn value were 0.The classifier correctly predicted 24 of them as 0, and one of them wrongly as 1. So, it has done a good job in predicting the customers with churn value 0. A good thing about confusion matrix is that shows the model’s ability to correctly predict or separate the classes. In specific case of binary classifier, such as this example, we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "\n",
    "* The 2nd column is for customers whose actual churn value in test set is 1. As you can calculate, out of 40 customers, the churn value of 15 of them is 1. And out of these 15, the classifier correctly predicted 6 of them as 1, and 9 of them as 0.It means, for 6 customers, the actual churn value were 1 in test set, and classifier also correctly predicted those as 1. However, while the actual label of 9 customers were 1, the classifier predicted those as 0, which is not very good. We can consider it as error of the model for first row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83        25\n",
      "           1       0.86      0.40      0.55        15\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.79      0.68      0.69        40\n",
      "weighted avg       0.78      0.75      0.72        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\\n\",classification_report(y_test, ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precision is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n",
    "\n",
    "* Recall is true positive rate. It is defined as: Recall =  TP / (TP + FN)\n",
    "\n",
    "* F1 score:is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. \n",
    "It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "* average accuracy for this classifier is the average of the F1-score for both labels, which is 0.75 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray of the logistic regression model is: 75.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuray of the logistic regression model is:\",round(accuracy_score(ypred,y_test)*100,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log loss( Logarithmic loss):\n",
    "measures the performance of a classifier where the predicted output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6017092478101187"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, ypred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the same model with the same dataset but with different __solver__ and __regularization__ values and calculating log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss: : 0.67\n"
     ]
    }
   ],
   "source": [
    "LR2 = LogisticRegression(C=0.001, solver='sag').fit(X_train,y_train)\n",
    "ypred_prob2 = LR2.predict_proba(X_test)\n",
    "print (\"LogLoss: : %.2f\" % log_loss(y_test, ypred_prob2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
